<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="隐马尔可夫模型（Hidden Markov Model，HMM）是马尔可夫过程的一个扩展，而马尔可夫过程是指满足马尔可夫假设的随机过程。马尔可夫假设是指一个随机变量的当前状态只依赖于前一个随机变量的状态，和其他的因素都无关。如下图所示： 其中 \(z_n\) 的值只受 \(z_{n-1}\) 的值影响。 然后隐马尔可夫模型再此基础上引入了这些状态的输出，也就是描述了由一个隐藏的马尔可夫链生成不">
<meta property="og:type" content="article">
<meta property="og:title" content="HMM推导及实现(一)">
<meta property="og:url" content="http://yoursite.com/2018/02/12/HMM推导及实现/index.html">
<meta property="og:site_name" content="Learn Machine Learning">
<meta property="og:description" content="隐马尔可夫模型（Hidden Markov Model，HMM）是马尔可夫过程的一个扩展，而马尔可夫过程是指满足马尔可夫假设的随机过程。马尔可夫假设是指一个随机变量的当前状态只依赖于前一个随机变量的状态，和其他的因素都无关。如下图所示： 其中 \(z_n\) 的值只受 \(z_{n-1}\) 的值影响。 然后隐马尔可夫模型再此基础上引入了这些状态的输出，也就是描述了由一个隐藏的马尔可夫链生成不">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/02/public/images/马尔科夫链.png">
<meta property="og:image" content="http://yoursite.com/2018/02/public/images/隐马尔可夫模型.png">
<meta property="og:image" content="http://yoursite.com/2018/02/public/images/前向-后向概率.png">
<meta property="og:image" content="http://yoursite.com/2018/02/public/images/EM.png">
<meta property="og:image" content="http://yoursite.com/2018/02/public/images/log.png">
<meta property="og:updated_time" content="2018-02-12T09:54:29.950Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HMM推导及实现(一)">
<meta name="twitter:description" content="隐马尔可夫模型（Hidden Markov Model，HMM）是马尔可夫过程的一个扩展，而马尔可夫过程是指满足马尔可夫假设的随机过程。马尔可夫假设是指一个随机变量的当前状态只依赖于前一个随机变量的状态，和其他的因素都无关。如下图所示： 其中 \(z_n\) 的值只受 \(z_{n-1}\) 的值影响。 然后隐马尔可夫模型再此基础上引入了这些状态的输出，也就是描述了由一个隐藏的马尔可夫链生成不">
<meta name="twitter:image" content="http://yoursite.com/2018/02/public/images/马尔科夫链.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/02/12/HMM推导及实现/"/>





  <title>HMM推导及实现(一) | Learn Machine Learning</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Learn Machine Learning</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">AI民间玩家</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/12/HMM推导及实现/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Spencer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Learn Machine Learning">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">HMM推导及实现(一)</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-12T17:52:18+08:00">
                2018-02-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>隐马尔可夫模型（Hidden Markov Model，HMM）是马尔可夫过程的一个扩展，而马尔可夫过程是指满足马尔可夫假设的随机过程。马尔可夫假设是指一个随机变量的当前状态只依赖于前一个随机变量的状态，和其他的因素都无关。如下图所示：<img src="../../public/images/马尔科夫链.png" alt="马尔科夫链"></p>
<p>其中 <span class="math inline">\(z_n\)</span> 的值只受 <span class="math inline">\(z_{n-1}\)</span> 的值影响。</p>
<p>然后隐马尔可夫模型再此基础上引入了这些状态的输出，也就是描述了由一个隐藏的马尔可夫链生成不可观测的状态随机序列，再由各个状态生成观测随机序列的过程。如下图所示：<img src="../../public/images/隐马尔可夫模型.png" alt="隐马尔可夫模型"></p>
<p>其中，<span class="math inline">\(z_1,z_2,\dots,z_{n-1},z_{n},z_{n+1}\)</span> 即不可观测的状态随机序列，它们是一个马尔可夫链，并且每个状态又生成了可观测的状态序列 <span class="math inline">\(x_1,x_2,\dots,x_{n-1},x_n,x_{n+1}\)</span>。而在现实数据中，存在很多这样的问题，比如大家都知道的中文分词问题，每个字都是一个可观测的值，它们组成了一个可观测序列，但是每个字是如何组成一个词的这个状态我们是无法直接观测到的，所以被称为隐变量，例如可以假设这里的隐变量有以下的状态序列空间 <span class="math inline">\(\{B,M,S,E\}\)</span>,其中<span class="math inline">\(B\)</span>表示这个字是一个词的开头组成，<span class="math inline">\(M\)</span>表示这个字是一个词的中间部分，<span class="math inline">\(S\)</span> 表示这个字是单独成词，而<span class="math inline">\(E\)</span>表示这个字在这个词的结束。如果是这样我们就能知道这个词是由哪些字组成的，比如“为人民服务”这个字序列对应的隐状态标记序列为“<span class="math inline">\(SBEBE\)</span>”，这样我们就能够达到中文分词的目的了。所以，当我们输入观测序列给HMM，它就能够给我们这个观测序列的标记序列，这个问题被称为HMM的预测问题，这里先不展开，后面再详细说。 &lt; !–more–&gt; 接下来，我们深入研究一下HMM的一些问题。通过上面的讲解，已经知道了隐变量有一个可能的状态序列，将其定义为 <span class="math inline">\(I=\{i_1,i_2,\dots,i_{T-1},i_{T} \}\)</span> （可以认为是HMM的Input，以后我们就把它叫做状态序列，<span class="math inline">\(T\)</span> 表示此时这个HMM的状态序列的长度），观测变量也有一个可能的取值，它们对应为状态序列 <span class="math inline">\(I\)</span>的输出，定义为 <span class="math inline">\(O=\{o_1,o_2,\dots,o_T \}\)</span>（可以认为是HMM的Output，称为观测序列）。需要注意的是，这里的 <span class="math inline">\(i_{index} 以及 o_{index}\)</span> 表示的是所有状态和观测集合的位置。我们还需要定义一个所有状态（隐变量）的集合为 <span class="math inline">\(Q=\{q_1,q_2,\dots,q_N\}\)</span> ，<span class="math inline">\(N\)</span> 表示所有状态的可能个数，所有可观测状态的集合为 <span class="math inline">\(V=\{v_1,v_2,\dots,v_M\}，M为所有可能的观测状态\)</span>。那么<span class="math inline">\(I\)</span>就是从状态集合<span class="math inline">\(Q\)</span>中抽取出来的序列，<span class="math inline">\(O\)</span>就是从观测集合<span class="math inline">\(V\)</span>中抽取出来的序列。</p>
<p>在这个HMM的图中，每一个状态都有一个初始概率，定义为一个向量 <span class="math inline">\(\pi = (\pi_i),\pi_i=P(i_1=q_i)\)</span>，表示时刻 <span class="math inline">\(t=1\)</span> 的状态为 <span class="math inline">\(q_i\)</span> 的概率。状态和状态之间都有一个依据时间的转移概率，比如状态 <span class="math inline">\(i_1\)</span>从时间 <span class="math inline">\(t\)</span>在下一个时间 <span class="math inline">\(t+1\)</span>变成状态 <span class="math inline">\(i_2\)</span> 会有一个概率，为了方便，我们记时刻<span class="math inline">\(t\)</span>时的状态为 <span class="math inline">\(q_t\)</span>，那么把这个转移概率表示成 <span class="math inline">\(a_{ij}=p(q_{t+1} = j | q_t = i)\)</span>，因为是转移，后一个状态依赖于前一个状态，所以这里是条件概率。然而，每一个状态从时刻 <span class="math inline">\(t\)</span> 可能会在下一个时刻 <span class="math inline">\(t+1\)</span> 转移到状态集合中的任何一个状态，状态一共有 <span class="math inline">\(N\)</span> 个，所以概率有 <span class="math inline">\(N \times N\)</span> 个，于是把所有的概率表示为一个概率矩阵 <span class="math inline">\(A = [a_{ij}]_{N \times N}\)</span>，这个矩阵被称为状态概率矩阵。同样地，我们的状态会在 <span class="math inline">\(t\)</span> 时刻生成一个观测，状态有 <span class="math inline">\(N\)</span> 个，观测有 <span class="math inline">\(M\)</span> 个，会得到一个 <span class="math inline">\(N \times M\)</span> 的观测概率矩阵 <span class="math inline">\(B=[b_{j}(k)]_{N \times M}\)</span>，其中 <span class="math inline">\(b_j(k) = P(o_t=v_k |i_t = q_j),k=1,2,\dots,M;j=1,2,\dots,N\)</span>，表示，在时刻 <span class="math inline">\(t\)</span>处于状态<span class="math inline">\(q_j\)</span>的条件下生成观测 <span class="math inline">\(v_k\)</span> 的概率。</p>
<p>我们把上面定义的三个参数 <span class="math inline">\(\lambda = (\pi,A,B)\)</span> 称为HMM的三要素，它们也是模型的参数，也就是说有了这个 <span class="math inline">\(\lambda\)</span>，我们就能够得到一个HMM了。由此，我们引出了HMM的三个问题： - 概率计算问题： 指在 <span class="math inline">\(\lambda\)</span> 和观测序列 <span class="math inline">\(O\)</span> 都已知的情况下，计算输出观测序列的概率，即求 <span class="math inline">\(P(O|\lambda)\)</span> - 学习问题： 指在观测序列 <span class="math inline">\(O\)</span> 已知的情况下，求得使得出现该观测序列概率最大的参数 <span class="math inline">\(\lambda\)</span>，即求 <span class="math inline">\(\lambda^* =\underset{\lambda}{\arg\max} P(O|\lambda)\)</span> - 预测问题： 指在 <span class="math inline">\(\lambda\)</span> 和观测序列 <span class="math inline">\(O\)</span> 都已知的情况下，计算最优可能的状态序列 <span class="math inline">\(I\)</span>,即求 <span class="math inline">\(I^*=\underset{I}{\arg\max }P(I|O;\lambda)\)</span></p>
<p>再进入这三个问题之前，先说说HMM是如何生成序列的，并且举一个HMM的例子，以熟悉上面定义的一些概念。关于生成序列，HMM是这样做的： - (1)使用初始概率向量 <span class="math inline">\(\pi\)</span> 产生状态 <span class="math inline">\(i_1\)</span>； - (2)令 <span class="math inline">\(t=1\)</span> - (3)按照状态 <span class="math inline">\(i_t\)</span> 的观测概率分布 <span class="math inline">\(b_{i_t}(k)\)</span> 生成 <span class="math inline">\(o_t\)</span> - (4)按照状态 <span class="math inline">\(i_t\)</span> 的状态转移概率分布 <span class="math inline">\(\{a_{i_ti_{t+1}}\}\)</span> 产生状态 <span class="math inline">\(i_{t+1}\)</span>,<span class="math inline">\(i_{t+1}=1,2,\dots,N\)</span> - (5) 令 <span class="math inline">\(t=t+1\)</span>；如果 <span class="math inline">\(t&lt;T\)</span>，转步(3)；否则，终止</p>
<p>下面举一个例子，假设有4个盒子，每个盒子里都装有红白两种颜色的球，盒子里的红白球数由下表给出。 盒子|1| 2| 3| 4 —–|—-|—-|—-|—- 红球数| 5| 3| 6|8 白球数|5|7|4|2 现在按照下面的方法抽球，产生一个球的颜色的观测序列：开始，从4个盒子里以等概率随机选取1个盒子，从这个盒子里随机抽出1个球，记录其颜色后，放回；然后，从当前盒子随机转移到下一个盒子，规则是：如果当前盒子是盒子1，那么下一盒子一定是盒子2，如果当前是盒子2或3，那么分别以概率0.4和0.6转移到左边或右边的盒子，如果当前是盒子4，那么各以0.5的概率停留在盒子4或转移到盒子3；确定转移的盒子后，再从这个盒子里随机抽出1个球，记录其颜色，放回；如此下去，重复进行5次，得到一个球的颜色的观测序列：<span class="math display">\[O=\{红，红，白，白，红\}\]</span>，在这个过程中，观察者只能观测到球的颜色的序列，观测不到球是从哪个盒子取出的，即观测不到盒子的序列。</p>
<p>在这个例子中有两个随机序列，一个是盒子的序列（状态序列），一个是球的颜色的观测序列（观测序列）。前者是隐藏的，只有后者是可观测的。这是一个隐马尔可夫模型的例子，根据所给条件，可以明确状态集合、观测集合、序列长度以及模型的三要素。</p>
<p>状态集合Q={盒子1，盒子2，盒子3，盒子4},N=4 观测集合V={红球，白球},M=2 由于是从4个盒子等概率随机选取1个盒子，所以初始概率向量为： <span class="math inline">\(\pi=[0.25,0.25,0.25,0.25]\)</span></p>
状态概率矩阵为： [ A =
<span class="math display">\[\begin{pmatrix}
    0 &amp; 1 &amp; 0 &amp; 0 \\
    0.4 &amp; 0 &amp; 0.6 &amp; 0 \\
    0 &amp; 0.4 &amp; 0 &amp; 0.6 \\
    0 &amp; 0 &amp; 0.5 &amp; 0.5 \\
    \end{pmatrix}
\]\]</span>
观测概率矩阵为： [ B =
<span class="math display">\[\begin{pmatrix}
    0.5 &amp; 0.5 \\
    0.3 &amp; 0.7 \\
    0.6 &amp; 0.4 \\
    0.8 &amp; 0.2 \\
    \end{pmatrix}
\]\]</span>
<h2 id="hmm的概率计算问题">HMM的概率计算问题</h2>
<p>下面就开始解决HMM最重要的三个问题了，首先是概率计算问题。有3种方法来计算这个问题： - 暴力计算 - 前向算法 - 后向算法</p>
<h3 id="暴力计算">暴力计算</h3>
<p>我们的问题是计算 <span class="math inline">\(P(O|\lambda)\)</span>。直接的思考方式是，先看看我们能计算出哪些概率，状态序列 <span class="math inline">\(I=(i_1,i_2,\dots,i_T)\)</span> 的概率是 <span class="math display">\[
P(I|\lambda)=\pi_{i_1}a_{i_1i_2}a_{i_2i_3} \dots a_{i_{T-1}i_{T}}
\]</span> 这个比较容易理解，首先产生 <span class="math inline">\(i_1\)</span> 状态，然后在 <span class="math inline">\(i_1\)</span>状态的条件下产生 <span class="math inline">\(i_2\)</span> 状态，通过转移概率不断向前推进。 再来看下 <span class="math inline">\(P(O|I,\lambda)\)</span>，它表示已知状态序列 <span class="math inline">\(I=(i_1,i_2,\dots,i_T)\)</span>，观测序列 <span class="math inline">\(O=(o_1,o_2,\dots,o_T)\)</span> 的概率。这个也可以直接算： <span class="math display">\[
P(O|I,\lambda)=b_{i_1}(o_1)b_{i_2}(o_2)\dots b_{i_{T}}(o_T)
\]</span> 这个也很直观，通过观测概率矩阵产生观测 <span class="math inline">\(o_1\)</span>，以此类推，直到产生所有的观测序列 <span class="math inline">\(O\)</span>。 然后，再让 <span class="math inline">\(P(O|\lambda)\)</span>和上面两个直观的结果看看能不能产生关系。我们发现： <span class="math display">\[
P(O|\lambda)=\sum_{I}P(O,I|\lambda)(别说你这步也看不懂)=\sum_{I}P(O|I,\lambda)P(I|\lambda)=\sum_{I}\pi_{i_1}a_{i_1i_2}a_{i_2i_3} \dots \\ a_{i_{T-1}i_{T}}b_{i_1}(o_1)b_{i_2}(o_2)\dots b_{i_{T}}(o_T)=\sum_{I}\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\dots a_{i_{T-1}i_{T}}b_{i_{T}}(o_T)
\]</span> 上面这个就是最终的观测序列的概率。这个式子通过一个所有状态序列 <span class="math inline">\(I\)</span> 的加和，内部是一个乘积的形式，一共有 <span class="math inline">\(2T\)</span> 个因子相乘，而外面的加和因为 <span class="math inline">\(I\)</span>的状态序列是从所有 <span class="math inline">\(N\)</span> 个状态集合中选出来的，即每个状态有 <span class="math inline">\(N\)</span>种选择，一共有 <span class="math inline">\(T\)</span> 个序列，所以一共有 <span class="math inline">\(N^T\)</span>，根据乘法法则那么一共要计算 <span class="math inline">\(2TN^T\)</span> 次，时间复杂度也就是 <span class="math inline">\(O(TN^T)\)</span>，是指数级的。所以，我们需要找更快的算法。</p>
<h3 id="前向算法">前向算法</h3>
<p>前向算法的思想是动态规划，通过递推的方式求得最终答案。定义一个前向概率，如下图：</p>
<p><img src="../../public/images/前向-后向概率.png" alt="前向-后向概率"> 左半边虚线是前向概率的定义，右半边虚线是后向概率的定义。无论是前向概率还是后向概率，都是通过状态和观测来定义的，前向概率是指划定一个时间 <span class="math inline">\(t\)</span>，此时状态 <span class="math inline">\(q_t = i\)</span>，观测必须是 <span class="math inline">\(\{y_1,y_2,\dots,y_{t-1},y_{t}\}\)</span>，满足这两种情况的概率就是前向概率，用数学的方法说就是 <span class="math display">\[
\alpha_t(i)=P(y_1,y_2,\dots,y_t,q_t=i|\lambda)
\]</span> 后向概率和前向概率基本差不多，也是划到 <span class="math inline">\(q_t\)</span>，只不过是向后看，但是不包括观测 <span class="math inline">\(y_t\)</span>，而是从 <span class="math inline">\(y_{t+1}\)</span> 开始，即要求观测是 <span class="math inline">\(y_{t+1},y_{t+2},\dots,y_{T}\)</span>，状态是 <span class="math inline">\(q_t = i\)</span>，即后向概率为： <span class="math display">\[
\beta_t(i) = P(y_{t+1},y_{t+2},\dots,y_{T} |q_t=i,\lambda)
\]</span> 直观地理解就是一个从前往后看状态和观测，一个时从后往前看状态和观测。 而前向算法主要是依据前向概率的递推过程，根据上面的推导，我们知道: <span class="math inline">\(\alpha_t(i)=P(o_1,o_2,\dots,o_t,q_t=i|\lambda)\)</span>，利用递推的方式，我们需要找到 <span class="math inline">\(t-1\)</span> 时的前向概率和 <span class="math inline">\(t\)</span> 时的前向概率之间的关系， <span class="math inline">\(t-1\)</span> 时的前向概率中，状态为 <span class="math inline">\(q_{t-1}=i\)</span>，观测为 <span class="math inline">\(\{o_1,o_2,\dots,o_{t-1}\}\)</span>，那么怎样才能把这个变成 <span class="math inline">\(\alpha_t(i)\)</span> 呢？那么需要将 <span class="math inline">\(q_{t-1} = i\)</span> 变成 <span class="math inline">\(q_{t} = i\)</span>，当然观测也要变。其实并不难，为了更加直观，把 <span class="math inline">\(\alpha_t(i)\)</span> 和 <span class="math inline">\(\alpha_{t-1}(i)\)</span> 分别写成概率的形式： - <span class="math inline">\(\alpha_t(i)=P(o_1,o_2,\dots,o_t,q_t=i|\lambda)\)</span> - <span class="math inline">\(\alpha_{t-1}(i)=P(o_1,o_2,\dots,o_{t-1},q_{t-1}=i|\lambda)\)</span></p>
<p>我们发现，<span class="math inline">\(q_{t-1}=i\)</span>无论如何都无法通过状态转移概率矩阵直接跳到 <span class="math inline">\(q_{t}=i\)</span> 的，因为状态都是 <span class="math inline">\(i\)</span>，所以就想到再加一个跳板，状态 <span class="math inline">\(j\)</span>，也就是说如果我们知道了 <span class="math inline">\(P(o_1,o_2,\dots,o_{t-1},q_{t-1}=j,q_t=i|\lambda)\)</span>，就可以通过求和把状态 <span class="math inline">\(j\)</span> 给消掉： <span class="math display">\[
P(o_1,o_2,\dots,o_{t-1},q_t=i|\lambda)=
\sum_j P(o_1,o_2,\dots,o_{t-1},q_{t-1}=j,q_t=i|\lambda)
\]</span> 而求和符号中的式子可以通过下面的方法得到: <span class="math display">\[
P(o_1,o_2,\dots,o_{t-1},q_{t-1}=j,q_t=i|\lambda)=P(o_1,o_2,\dots,o_{t-1},q_{t-1}=j)a_{ji}=\alpha_j(t-1)a_{ji}
\]</span></p>
<p>而 <span class="math display">\[
P(o_1,o_2,\dots,o_{t},q_t=i|\lambda)=P(o_1,o_2,\dots,o_{t-1},q_t=i|\lambda)b_{i}(o_t)
\]</span> 最终得到： <span class="math display">\[
\alpha_t(i) = \sum_{j=1}^{N}\Big(\alpha_{t-1}(j)a_{ji}\Big)b_i(o_t)（注意区分 \alpha 和 a ）
\]</span></p>
<p>具体的递推过程为： - <span class="math inline">\(初值：\alpha_1(i)=\pi_{i}b_{i}(o_1)\)</span> - <span class="math inline">\(递推：对于t=1,2,\dots,T-1 \\ \alpha_{t+1}(i) = \sum_{j=1}^{N}\Big(\alpha_{t}(j)a_{ji}\Big)b_i(o_{t+1})\)</span> - <span class="math inline">\(最终：P(O|\lambda)=\sum_{i=1}^N\alpha_T(i)\)</span></p>
<h3 id="后向算法">后向算法</h3>
<p>后向算法的推导基本和前向算法一样，只是后向算法是从后向前算，初值为 <span class="math inline">\(\beta_T(i)=1\)</span>，因为后面没有了。 递推： <span class="math display">\[
对于t=T-1,T-2,\dots,1 \\
\beta_t(i) = \sum_{j=1}^N\Big(a_{ij}b_j(o_{t+1})\beta_{i+1}(j)\Big)
\]</span> 最终： <span class="math display">\[
P(O|\lambda)=\sum_{i=1}^N\pi_ib_i(o_1)\beta_1(i)
\]</span></p>
<h3 id="前后向概率的关系">前后向概率的关系</h3>
<p>由上得知前向概率为: <span class="math display">\[
\alpha_t(i)=P(o_1,o_2,\dots,o_t,q_t=i|\lambda)
\]</span></p>
<p>后向概率为： <span class="math display">\[
\beta_t(i)=P(o_{t+1},o_{t+2},\dots,o_T|q_t=i,\lambda)
\]</span></p>
<p>那么我们来算一下： <span class="math display">\[
P(q_t=i,O|\lambda)=P(O|q_t=i,\lambda)P(q_t=i|\lambda)\\=P(o_1,\dots,o_t|q_t=i,\lambda)P(q_t=i|\lambda)\\=P(o_1,\dots,o_t,o_{t+1},\dots o_T|q_t=i,\lambda)P(q_t=i|\lambda)\\
=P(o_1,\dots,o_t|q_t=i,\lambda)P(o_{t+1},\dots,o_T|q_t=i,\lambda)P(q_t=i|\lambda)\\=P(o_1,\dots,o_t,q_t=i|\lambda)P(o_{t+1},\dots,o_T|q_t=i,\lambda)\\=\alpha_t(i)\beta_t(i)
\]</span> 即 <span class="math display">\[
P(q_t=i,O|\lambda)=\alpha_t(i)\beta_t(i)
\]</span></p>
<p>求给定模型 <span class="math inline">\(\lambda\)</span>和观测 <span class="math inline">\(O\)</span>，在时刻<span class="math inline">\(t\)</span>处于状态 <span class="math inline">\(q_t=i\)</span>的概率： <span class="math display">\[
\gamma_t(i)=P(q_t=i|O,\lambda)
\]</span> 上面的式子被称为单个状态的概率。 根据前向后向概率的定义， <span class="math display">\[
P(q_t=i,O|\lambda)=\alpha_t(i)\beta_t(i) \\
\gamma_t(i)=P(q_t=i|O,\lambda)=\frac{P(q_t=i,O|\lambda)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum_{i=1}^N\alpha_t(i)\beta_t(i)}
\]</span></p>
<p><span class="math inline">\(\gamma\)</span>的含义是指，在每个时刻<span class="math inline">\(t\)</span>选择在该时刻最有可能出现的状态<span class="math inline">\(i_t^*\)</span>，从而得到一个状态序列<span class="math inline">\(I^*=\{i_1^*,i_2^*,\dots,i_T^*\}\)</span>，将它作为预测的结果。</p>
<p>另外还有，两个状态的联合概率，即求给定模型 <span class="math inline">\(\lambda\)</span>和观测<span class="math inline">\(O\)</span>，在时刻<span class="math inline">\(t\)</span>处于状态<span class="math inline">\(q_t=i\)</span>并且时刻<span class="math inline">\(t+1\)</span>处于状态<span class="math inline">\(q_{t+1}=j\)</span>的概率。 <span class="math display">\[
\xi_t(i,j)=P(q_t=i,q_{t+1}=j|O,\lambda)=\frac{P(q_t=i,q_{t+1}=j,O|\lambda)}{P(O|\lambda)}\\=\frac{P(q_t=i,q_{t+1}=j,O|\lambda)}{\sum_{i=1}^{N}\sum_{j=1}^{N}P(q_t=i,q_{t+1}=j,O|\lambda)}\\P(q_t=i,q_{t+1}=j,O|\lambda)=\alpha_t(i)a_{ij}b_{j}(o_{t+1}\beta_{t+1}(j))
\]</span> 到达这里，没有什么问题吧？下面还有几个期望求一下： - 在观测<span class="math inline">\(o\)</span>下状态<span class="math inline">\(i\)</span>出现的期望(概率求和)： <span class="math display">\[
\sum_{t=1}^T\gamma_t(i)
\]</span> - 在观测<span class="math inline">\(O\)</span>下状态<span class="math inline">\(i\)</span>转移到状态<span class="math inline">\(j\)</span>的期望： <span class="math display">\[
\sum_{t=1}^{T-1}\xi_t(i,j)
\]</span> 这些公式后面会用到。至此，我们解决了HMM的概率计算问题。接下来时HMM的学习问题，即如何通过数据学得模型的参数 <span class="math inline">\(\lambda\)</span>。 ## HMM的学习问题 HMM的学习问题主要也有两个算法： - 有监督学习算法：数据包括观测序列和状态序列 - 鲍姆韦尔奇算法：数据只有观测序列 ### 有监督学习算法 估计参数最常用的是用极大似然估计，假设训练数据包含 <span class="math inline">\(S\)</span>个长度相同的观测序列和对应的状态序列 <span class="math inline">\(\{(O_1,I_1),(O_2,I_2),(O_3,I_3),\dots,(O_S,I_S)\}\)</span>。</p>
<ul>
<li><p>转移概率 <span class="math inline">\(a_{ij}\)</span> 的估计<br>
设样本中时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(i\)</span> 时刻 <span class="math inline">\(t+1\)</span> 转移到状态 <span class="math inline">\(j\)</span>的频数为 <span class="math inline">\(A_{ij}\)</span>，那么状态转移概率 <span class="math inline">\(a_{ij}\)</span> 的估计是： <span class="math display">\[
\hat{a}_{ij}=\frac{A_{ij}}{\sum_{j=1}^NA_{ij}}
\]</span></p></li>
<li><p>观测概率 <span class="math inline">\(b_{i}(k)\)</span> 的估计<br>
设样本中状态为 <span class="math inline">\(j\)</span> 并且观测为 <span class="math inline">\(k\)</span> 的频数是 <span class="math inline">\(B_{jk}\)</span>，那么状态为 <span class="math inline">\(j\)</span> 观测为 <span class="math inline">\(k\)</span> 的概率 <span class="math inline">\(b_j(k)\)</span> 的估计是： <span class="math display">\[
\hat{b}_j(k)=\frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}}
\]</span></p></li>
<li><p>初始状态概率 <span class="math inline">\(\pi_i\)</span> 的估计 <span class="math inline">\(\hat{\pi}_i\)</span>为 <span class="math inline">\(S\)</span> 个样本中初始状态为 <span class="math inline">\(q_i\)</span> 的频率。 ### 鲍姆韦尔奇（Baum-Welch）算法 假设给定训练数据只包含 <span class="math inline">\(S\)</span> 个长度为 <span class="math inline">\(T\)</span> 的观测序列 <span class="math inline">\(\{O_1,O_2,\dots,O_S\}\)</span> 而没有对应的状态序列，目标时学习隐马尔可夫模型 <span class="math inline">\(\lambda=(A,B,\pi)\)</span> 的参数。我们将观测序列数据看作观测数据 <span class="math inline">\(O\)</span>，状态序列数据看作不可观测的隐数据 <span class="math inline">\(I\)</span>，那么隐马尔科夫模型事实上是一个含有隐变量的概率模型 <span class="math display">\[
P(O|\lambda)=\sum_IP(O|I,\lambda)P(I|\lambda)
\]</span> 它的参数可以由EM算法实现。下面介绍一下EM算法的由来和推导。</p></li>
</ul>
<h4 id="em算法">EM算法</h4>
<p>一句话，EM算法是用来寻找含有隐变量的模型的参数的。假设有训练集 <span class="math inline">\(\{x^{(1)},x^{(2)},\dots,x^{(m)}\}\)</span>，包含 <span class="math inline">\(m\)</span> 个独立样本，希望从中找到该组数据的模型 <span class="math inline">\(p(x,z)\)</span> 的参数。思想也很简单，通过极大似然估计法，对观测到的数据<span class="math inline">\(x\)</span>取对数似然函数: <span class="math display">\[
l(\theta)=\sum_{i=1}^m\log p(x;\theta)=\sum_{i=1}^m\log\sum_zp(x,z;\theta)
\]</span> 而 <span class="math inline">\(z\)</span>时隐随机变量，不方便直接找到参数估计。于是前辈们想出了一个策略：计算 <span class="math inline">\(l(\theta)\)</span> 下界，求该下界的最大值；重复该过程，直到收敛到局部最大值。请看下面这张图：</p>
<p><img src="../../public/images/EM.png"></p>
<p>紫色的 <span class="math inline">\(l(\theta)\)</span> 函数，我们先找到一个函数 <span class="math inline">\(r(x|\theta)\)</span>去迭代逼近 <span class="math inline">\(p(x|\theta)\)</span>，先给定一个初值（绿色线），再求出这个 <span class="math inline">\(r(x|\theta)\)</span> 的最大值，并且保证 <span class="math inline">\(r(x|\theta)\)</span> 函数值比 <span class="math inline">\(p(x|\theta)\)</span> 函数值小，下一次就用红色的线代替原来的初值不断迭代，直到收敛到一个局部的最大值。接下来推导一下，推导过程中使用到了一个著名的不等式-Jensen不等式，简单来说，就是凸函数的割线在该函数的上方，并且可以扩展到概率期望上。这里暂时先不证明了，有机会再完善，只要记住：f是一个凸函数，<span class="math inline">\(X\)</span>随机变量，则： <span class="math display">\[
E[f(X)] &gt;=f(EX)
\]</span> 函数的期望大于等于期望的函数。</p>
<p><span class="math display">\[l(\theta)=\sum_{i=1}^m\log \sum_{z} p(x,z;\theta)=\sum_{i=1}^m \log \sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)=\sum_{i=1}^m \log \sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
\]</span> 上面我们假设了 <span class="math inline">\(Q_i\)</span> 是 <span class="math inline">\(z\)</span>的某一个分布，<span class="math inline">\(Q_i &gt;= 0\)</span>。在最后一个式子中，我们可以把 <span class="math inline">\(\log \sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\)</span> 看成是一个对数函数，并且把 <span class="math inline">\(\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\)</span>看成是一个随机变量 <span class="math inline">\(X\)</span>，那么这个求和就当于 <span class="math inline">\(X\)</span> 在分布 <span class="math inline">\(Q_i(z^{(i)})\)</span> 上的期望，也就是这里做了期望的函数操作，而对数是凹函数，那么根据上面的Jensen不等式有凹函数：期望的函数大于等于函数的期望。所以得到： <span class="math display">\[
l(\theta)=\sum_{i=1}^m\log \sum_{z} p(x,z;\theta)=\sum_{i=1}^m \log \sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)=\sum_{i=1}^m \log \sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \\ \geq  \sum_{i=1}^m \sum_{z^{(i)}} Q_i(z^{(i)}) \log  \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
\]</span> 于是我们就找到了下界，那么这个等号什么时候成立呢？我们来看看对数函数的图像：</p>
<p><img src="../../public/images/log.png"> 一般来说函数的值都是大于割线的值，只有当两点 <span class="math inline">\(x_1=x_2\)</span>时，对数函数的值才等于割线的值。那么如果这个随机变量 <span class="math inline">\(X\)</span> 取一个定值 <span class="math inline">\(C\)</span>，就可以达到 <span class="math inline">\(x_1=x_2\)</span> 的目的。于是我们得到下面的式子： <span class="math display">\[
\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} = C
\]</span></p>
<p>也就是说 <span class="math display">\[
Q_i(z^{(i)}) \propto p(x^{(i)},z^{(i)};\theta)
\]</span> <span class="math inline">\(Q_i(z^{(i)})\)</span> 是一个分布，加和应该为1，如果希望把正比符号变成等号，那么 <span class="math display">\[
Q_i(z^{(i)})=\frac{p(x^{(i)},z^{(i)};\theta)}{\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)}=\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)} = p(z^{(i)}|x^{(i)};\theta)
\]</span> 也就是说 <span class="math inline">\(Q_i(z^{(i)})\)</span> 取隐变量 <span class="math inline">\(z\)</span> 的条件概率时等号成立，这样可以得到一个紧确的解（时间关系，里面还有一些细节没有写出来，有机会再完善）。有了这个条件概率的式子，那么我们就能一开始给一个 <span class="math inline">\(\theta= \theta_1\)</span> 的初值，把这个条件概率算出来，算完之后把它带到对数似然函数中（此时的 <span class="math inline">\(\theta\)</span>）是未知的，然后求对数似然函数的极大值的那个 <span class="math inline">\(\theta = \theta^*\)</span>，算完之后，再用这个 <span class="math inline">\(\theta=\theta^*\)</span> 代入到 <span class="math inline">\(Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\theta)\)</span>，继续算 <span class="math inline">\(Q_i(z^{(i)})\)</span>，这样不断重复，直到 <span class="math inline">\(\theta\)</span> 收敛到一个稳定的值，就是我们要求的参数了。关于EM算法的收敛证明，暂时就不展开了。在进入鲍姆韦尔奇算法之前，再补充下EM算法相关东西，我们求到的条件概率分布 <span class="math inline">\(Q_i(z^{(i)})\)</span> 代入到极大化的那个式子实际上是求期望，求解的极大可以看成是对数似然函数关于这个 <span class="math inline">\(Q_i(z^{(i)})\)</span> 分布的期望，所以就有了江湖中传闻的 <span class="math inline">\(E\)</span> 步，有了这个就可以极大化了。我们可以把期望写成： <span class="math display">\[
Q(\theta,\theta^{(i)})=E_{z} = \sum_{z^{(i)}}p(z^{(i)}|x^{(i)},\theta^{(i)})\log p(x^{(i)},z^{(i)};\theta)
\]</span> 嗯，这样就完美了。其中，<span class="math inline">\(\theta^{(i)}\)</span> 是已知的，初值是随机给定的，<span class="math inline">\(\theta\)</span> 是我们每次迭代要求解的。极大化被称作M步。 有了EM算法，我们就能在鲍姆韦尔奇算法中求解HMM的参数了，前面已经说过，HMM参数的求解被认为是一个含隐变量的模型参数。 - 确定完全数据的对数似然函数<br>
所有观测数据写成 <span class="math inline">\(O=(o_1,o_2,\dots,o_T)\)</span> ，所有隐数据写成 <span class="math inline">\(I=(i_1,i_2,\dots,i_T)\)</span> ，完全数据是 <span class="math inline">\((O,I)=(o_1,o_2,\dots,o_T,i_1,i_2,\dots,i_T)\)</span> 。完全数据的对数似然函数是 <span class="math inline">\(\log P(O,I|\lambda)\)</span> 。 - E步<br>
求 <span class="math inline">\(Q\)</span> 函数 <span class="math inline">\(Q(\lambda,\overline{\lambda})\)</span>: <span class="math display">\[
Q(\lambda,\overline{\lambda})=\sum_{I}\log \Big(P(O,I|\lambda)\Big)P(I|O,\overline{\lambda})
\\=\sum_{I}\log \Big(P(O,I|\lambda)\Big)\frac{P(O,I|\overline{\lambda})}{P(O|\overline{\lambda})}\propto \sum_{I}\log \Big(P(O,I|\lambda)\Big)P(O,I|\overline{\lambda})\]</span> 其中，<span class="math inline">\(\overline{\lambda}\)</span> 是HMM当前估计值，<span class="math inline">\(\lambda\)</span> 是要极大化的HMM参数。 根据 <span class="math inline">\(P(O,I|\lambda)=P(O|I,\lambda)P(I|\lambda) =\pi_{i_1}b_{i_1}(o_1)a_{i_1,i_2}b_{i_2}(o_2)\dots a_{i_{T-1}i_T}b_{i_T}(o_T)\)</span></p>
<p>函数可写成三部分的加和： <span class="math display">\[
Q(\lambda,\overline{\lambda})=\sum_I\log P(O,I|\lambda)P(O,I|\overline{\lambda})=\sum_I\log \pi_{i_1}P(O,I|\overline{\lambda})\\+\sum_I\Big(\sum_{t=1}^{T-1}\log a_{i_ti_{t+1}}\Big)P(O,I|\overline{\lambda}) + \sum_I\Big(\sum_{t=1}^{T}\log b_{i_t}(o_t)\Big)P(O,I|\overline{\lambda})
\]</span> 分别对它们使用拉格朗日乘子法极大化得到（以后再补充细节）： - 初始状态概率: <span class="math inline">\(\pi_i=\frac{P(O,q_1=i|\overline{\lambda})}{P(O|\overline{\lambda})}=\frac{P(O,q_1=i|\overline{\lambda})}{\sum_{i=1}^N P(O,q_1=i|\overline{\lambda})}=\gamma_1(i)\)</span></p>
<ul>
<li>观测概率： <span class="math display">\[
a_{ij}=\frac{\sum_{t=1}^{T-1}P(O,q_t=i,q_{t+1}=j|\overline(\lambda))}{\sum_{t=1}^{T-1}P(O,q_t=i|\overline{\lambda})}=\frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)}
\]</span></li>
<li>转移概率： <span class="math display">\[ b_{ik}=\frac{\sum_{t=1}^TP(O,q_t=i|\overline{\lambda})I(o_t=v_k)}{\sum_{t=1}^TP(O,q_t=i|\overline{\lambda})}=\frac{\sum_{t=1,o_t=v_k}^T\gamma_t(i)}{\sum_{t=1}^T\gamma_t(i)}
\]</span> 到此，HMM的学习问题就解决了。接下来是最后一个问题，关于HMM的预测问题。</li>
</ul>
<h2 id="hmm的预测问题">HMM的预测问题</h2>
<h3 id="近似算法">近似算法</h3>
<p>略过</p>
<h3 id="维特比算法">维特比算法</h3>
<p>维特比算法实际是用动态规划解HMM的预测问题，即用DP求概率最大路径。这时一条路径对应着一个状态序列。</p>
<p>根据DP原理，最优路径具有这样的性质：如果最优路径在时刻 <span class="math inline">\(t\)</span> 通过结点 <span class="math inline">\(i_t^*\)</span>，那么这一路径从结点 <span class="math inline">\(i_t^*\)</span> 到终点 <span class="math inline">\(i_T^*\)</span> 的部分路径，对于从 <span class="math inline">\(i_t^*\)</span> 到 <span class="math inline">\(i_T^*\)</span> 的所有可能的部分路径来说，必须是最优的。因为假如不是这样，那么从 <span class="math inline">\(i_t^*\)</span> 到 <span class="math inline">\(i_T^*\)</span> 就有另一条更好的部分路径存在，如果把它和从 <span class="math inline">\(i_1^*\)</span> 到达 <span class="math inline">\(i_t*\)</span> 的部分路径连接起来，就会形成一条比原来的路径更优的路径，这是矛盾的。依据这一原理，我们只需从时刻 <span class="math inline">\(t=1\)</span> 开始递推地计算在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span>的各条部分路径的最大概率，直至得到时刻 <span class="math inline">\(t=T\)</span> 状态为 <span class="math inline">\(i\)</span> 的各条路径的最大概率。时刻 <span class="math inline">\(t=T\)</span> 的最大概率即为最优路径的概率 <span class="math inline">\(P^*\)</span>，最优路径的终结点 <span class="math inline">\(i_{T=1}^*\)</span> 也同时得到。之后，为了找出最优路径的各个结点，从终结点 <span class="math inline">\(i_T^*\)</span> 开始，由后向前逐步求得结点 <span class="math inline">\(i_{T-1}^*,\dots,i_1^*\)</span>，得到最优路径 <span class="math inline">\(I^*=(i_1^*,i_2^*,\dots,i_T^*)\)</span>。这就是维特比算法。</p>
<p>定义一个变量 <span class="math inline">\(\delta_t(i)\)</span>：在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span> 的所有路径中，概率的最值。具体的算法过程为： - 定义： <span class="math display">\[
\delta_t(i)= \underset{i_1,i_2,\dots,i_{t-1}}{\max}P(q_t=i,i_{t-1},\dots,i_1,o_t,\dots,o_1|\lambda)
\]</span> - 递推： <span class="math display">\[
\delta_1(i)=\pi_ib_i(o_1)\\
\delta_{t+1}(i)=\underset{i_1,i_2,\dots,i_{t}}{\max}P(q_t=i,i_{t},\dots,i_1,o_t,\dots,o_1|\lambda)\\=\underset{1\leq j \leq N}{\max}\Big(\delta_t(j)a_{ji}b_i(o_{t+1})\Big)
\]</span></p>
<ul>
<li>终止： <span class="math display">\[
P^*=\underset{1\leq i \leq N}{\max}\delta_T(i)
\]</span></li>
</ul>
<h1 id="参考文献">参考文献</h1>
<p>李航 《统计学习方法》,2012.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/04/TensorFlow基础知识/" rel="next" title="TensorFlow基础知识">
                <i class="fa fa-chevron-left"></i> TensorFlow基础知识
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/head.jpg"
                alt="Spencer" />
            
              <p class="site-author-name" itemprop="name">Spencer</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#hmm的概率计算问题"><span class="nav-number">1.</span> <span class="nav-text">HMM&#x7684;&#x6982;&#x7387;&#x8BA1;&#x7B97;&#x95EE;&#x9898;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#暴力计算"><span class="nav-number">1.1.</span> <span class="nav-text">&#x66B4;&#x529B;&#x8BA1;&#x7B97;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向算法"><span class="nav-number">1.2.</span> <span class="nav-text">&#x524D;&#x5411;&#x7B97;&#x6CD5;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#后向算法"><span class="nav-number">1.3.</span> <span class="nav-text">&#x540E;&#x5411;&#x7B97;&#x6CD5;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前后向概率的关系"><span class="nav-number">1.4.</span> <span class="nav-text">&#x524D;&#x540E;&#x5411;&#x6982;&#x7387;&#x7684;&#x5173;&#x7CFB;</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#em算法"><span class="nav-number">1.4.1.</span> <span class="nav-text">EM&#x7B97;&#x6CD5;</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hmm的预测问题"><span class="nav-number">2.</span> <span class="nav-text">HMM&#x7684;&#x9884;&#x6D4B;&#x95EE;&#x9898;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#近似算法"><span class="nav-number">2.1.</span> <span class="nav-text">&#x8FD1;&#x4F3C;&#x7B97;&#x6CD5;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#维特比算法"><span class="nav-number">2.2.</span> <span class="nav-text">&#x7EF4;&#x7279;&#x6BD4;&#x7B97;&#x6CD5;</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考文献"><span class="nav-number"></span> <span class="nav-text">&#x53C2;&#x8003;&#x6587;&#x732E;</span></a></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Spencer</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
